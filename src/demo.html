<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WebGPU Image Processing Demo</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            overscroll-behavior: none; /* Prevents pull-to-refresh on mobile */
        }
        /* Custom scrollbar for a more material feel - optional */
        ::-webkit-scrollbar {
            width: 8px;
            height: 8px;
        }
        ::-webkit-scrollbar-track {
            background: #1e293b; /* slate-800 */
        }
        ::-webkit-scrollbar-thumb {
            background: #6366f1; /* indigo-500 */
            border-radius: 4px;
        }
        ::-webkit-scrollbar-thumb:hover {
            background: #4f46e5; /* indigo-600 */
        }
        .placeholder-bg {
            background-image: repeating-linear-gradient(
                45deg,
                rgba(255, 255, 255, 0.05),
                rgba(255, 255, 255, 0.05) 10px,
                rgba(255, 255, 255, 0.08) 10px,
                rgba(255, 255, 255, 0.08) 20px
            );
        }
        /* Ensure canvas is not display: none initially, but placeholder text handles visibility */
        #webgpuCanvas {
            display: block; /* Or 'inline-block' depending on layout needs */
        }
    </style>
    <script>
        // Tailwind config
        tailwind.config = {
            darkMode: 'class',
            theme: {
                extend: {
                    colors: {
                        'primary': {
                            DEFAULT: '#6366f1', // indigo-500
                            hover: '#4f46e5'  // indigo-600
                        },
                        'surface': '#1e293b', // slate-800 (Darker surface for main content area)
                        'surface-container': '#334155', // slate-700 (Slightly lighter for containers like image previews)
                        'on-surface': '#e2e8f0', // slate-200
                        'outline-variant': '#475569', // slate-600
                        'background-page': '#0f172a' // slate-900 (Overall page background)
                    },
                }
            }
        }
    </script>
</head>
<body class="bg-background-page text-on-surface min-h-screen flex flex-col items-center justify-center p-4 selection:bg-primary selection:text-white">

    <div class="w-full max-w-3xl bg-surface shadow-2xl rounded-xl p-6 md:p-8 space-y-6">
        <header class="text-center">
            <h1 class="text-3xl font-bold text-transparent bg-clip-text bg-gradient-to-r from-purple-400 via-pink-500 to-red-500">WebGPU Image Processor</h1>
            <p class="text-slate-400 mt-2">Demo: Grayscale Filter</p>
        </header>

        <div id="webgpu-status" class="p-3 rounded-md text-sm text-center font-medium"></div>

        <section class="space-y-4">
            <div>
                <label for="imageUpload" class="block text-sm font-medium text-slate-300 mb-1">Upload Image:</label>
                <input type="file" id="imageUpload" accept="image/*"
                       class="block w-full text-sm text-slate-400 file:mr-4 file:py-2 file:px-4
                              file:rounded-lg file:border-0 file:text-sm file:font-semibold
                              file:bg-primary file:text-white hover:file:bg-primary-hover
                              cursor-pointer border border-outline-variant rounded-lg focus:outline-none focus:ring-2 focus:ring-primary focus:border-primary">
            </div>

            <button id="processButton" disabled
                    class="w-full bg-primary hover:bg-primary-hover text-white font-semibold py-3 px-4 rounded-lg shadow-md
                           transition-all duration-150 ease-in-out focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-offset-surface focus:ring-primary
                           disabled:opacity-50 disabled:cursor-not-allowed">
                Apply Grayscale Filter
            </button>
        </section>

        <section class="grid grid-cols-1 md:grid-cols-2 gap-4 items-start">
            <div>
                <h2 class="text-lg font-semibold mb-2 text-slate-300">Original Image:</h2>
                <div class="aspect-square bg-surface-container rounded-lg overflow-hidden shadow flex items-center justify-center placeholder-bg">
                    <img id="originalImage" src="#" alt="Original" class="max-w-full max-h-full object-contain hidden">
                    <p id="originalPlaceholder" class="text-slate-500 p-4 text-center">No image loaded</p>
                </div>
            </div>
            <div>
                <h2 class="text-lg font-semibold mb-2 text-slate-300">Processed Image (WebGPU):</h2>
                <div class="aspect-square bg-surface-container rounded-lg overflow-hidden shadow flex items-center justify-center placeholder-bg">
                    <canvas id="webgpuCanvas"></canvas> {/* Canvas itself should not be hidden by default if placeholder text is used */}
                    <p id="canvasPlaceholder" class="text-slate-500 p-4 text-center">Output will appear here</p>
                </div>
            </div>
        </section>
    </div>

    <footer class="text-center mt-8 text-slate-500 text-sm">
        <p>WebGPU is a new technology. Ensure your browser supports it and hardware acceleration is enabled.</p>
        <p>&copy; 2025 AI Image Demo</p>
    </footer>

    <script>
        // --- Global WebGPU Variables ---
        let device = null;
        let context = null;
        let pipeline = null;
        let sampler = null;
        let sourceImageBitmap = null;
        let sourceTexture = null;
        let bindGroup = null; // This will hold the bind group for our resources

        // --- UI Elements ---
        const imageUpload = document.getElementById('imageUpload');
        const processButton = document.getElementById('processButton');
        const originalImage = document.getElementById('originalImage');
        const originalPlaceholder = document.getElementById('originalPlaceholder');
        const webgpuCanvas = document.getElementById('webgpuCanvas');
        const canvasPlaceholder = document.getElementById('canvasPlaceholder');
        const webgpuStatus = document.getElementById('webgpu-status');

        // --- WGSL Shaders ---
        const vertexShaderWGSL = `
            // This struct defines the output of our vertex shader.
            // @builtin(position) is the clip space position.
            // @location(0) is a user-defined location for passing data to the fragment shader (texture coordinates).
            struct VertexOutput {
                @builtin(position) position : vec4<f32>,
                @location(0) tex_coord : vec2<f32>,
            }

            // The entry point for the vertex shader.
            // @builtin(vertex_index) is an input that tells us which vertex we are processing.
            @vertex
            fn vs_main(@builtin(vertex_index) vertex_index : u32) -> VertexOutput {
                // We are drawing a full-screen quad using two triangles.
                // These are Normalized Device Coordinates (NDC) for the vertices.
                // Triangle 1: (0,1,2), Triangle 2: (2,1,3) -> (TopLeft, BottomLeft, TopRight) and (TopRight, BottomLeft, BottomRight)
                // To cover the screen, we use 6 vertices.
                var pos = array<vec2<f32>, 6>(
                    vec2<f32>(-1.0,  1.0),  // Top-left
                    vec2<f32>(-1.0, -1.0),  // Bottom-left
                    vec2<f32>( 1.0,  1.0),  // Top-right

                    vec2<f32>( 1.0,  1.0),  // Top-right (repeated for second triangle)
                    vec2<f32>(-1.0, -1.0),  // Bottom-left (repeated)
                    vec2<f32>( 1.0, -1.0)   // Bottom-right
                );

                // Corresponding texture coordinates (UVs) for each vertex.
                // (0,0) is top-left in texture space, (1,1) is bottom-right.
                var uvs = array<vec2<f32>, 6>(
                    vec2<f32>(0.0, 0.0), // Top-left
                    vec2<f32>(0.0, 1.0), // Bottom-left
                    vec2<f32>(1.0, 0.0), // Top-right

                    vec2<f32>(1.0, 0.0), // Top-right (repeated)
                    vec2<f32>(0.0, 1.0), // Bottom-left (repeated)
                    vec2<f32>(1.0, 1.0)  // Bottom-right
                );

                var output : VertexOutput;
                output.position = vec4<f32>(pos[vertex_index], 0.0, 1.0); // Set Z to 0, W to 1
                output.tex_coord = uvs[vertex_index];
                return output;
            }
        `;

        const fragmentShaderWGSL_Grayscale = `
            // Define resources that the fragment shader will use.
            // @group(0) @binding(0) declares a sampler in bind group 0 at binding 0.
            // @group(0) @binding(1) declares a 2D texture in bind group 0 at binding 1.
            @group(0) @binding(0) var mySampler: sampler;
            @group(0) @binding(1) var inputTexture: texture_2d<f32>;

            // The entry point for the fragment shader.
            // @location(0) receives the texture coordinates from the vertex shader.
            // It returns a vec4<f32> which is the color for the current fragment (pixel).
            @fragment
            fn fs_main(@location(0) tex_coord : vec2<f32>) -> @location(0) vec4<f32> {
                // Sample the input texture at the given texture coordinates using the sampler.
                let original_color = textureSample(inputTexture, mySampler, tex_coord);
                
                // Apply a standard luminance-preserving grayscale formula.
                // gray = 0.299 * R + 0.587 * G + 0.114 * B
                let gray = original_color.r * 0.299 + original_color.g * 0.587 + original_color.b * 0.114;
                
                // Return the grayscale color, preserving the original alpha.
                return vec4<f32>(gray, gray, gray, original_color.a);
            }
        `;

        // --- WebGPU Initialization and Setup ---
        async function initWebGPU() {
            // Check if the browser supports WebGPU
            if (!navigator.gpu) {
                updateStatus("WebGPU not supported on this browser. Please use a modern browser like Chrome or Edge.", "error");
                processButton.disabled = true;
                return false;
            }

            try {
                // Request a GPU adapter
                const adapter = await navigator.gpu.requestAdapter();
                if (!adapter) {
                    updateStatus("Failed to get GPU adapter. Make sure hardware acceleration is enabled.", "error");
                    processButton.disabled = true;
                    return false;
                }

                // Request a GPU device from the adapter
                device = await adapter.requestDevice();
                if (!device) {
                    updateStatus("Failed to get GPU device.", "error");
                    processButton.disabled = true;
                    return false;
                }

                // Handle device loss
                device.lost.then((info) => {
                    console.error(`WebGPU device was lost: ${info.message}`);
                    updateStatus(`WebGPU device lost: ${info.message}. Please reload or try again.`, "error");
                    processButton.disabled = true; // Disable processing if device is lost
                    // Potentially re-initialize or disable UI further
                    if (info.reason !== 'destroyed') {
                        // This was an unexpected loss.
                    }
                });
                
                // Get the WebGPU rendering context from the canvas
                context = webgpuCanvas.getContext('webgpu');
                if (!context) {
                    updateStatus("Failed to get WebGPU context from canvas.", "error");
                    processButton.disabled = true;
                    return false;
                }

                // Get the preferred canvas format for optimal display
                const presentationFormat = navigator.gpu.getPreferredCanvasFormat();

                // Configure the canvas context
                context.configure({
                    device: device,
                    format: presentationFormat,
                    alphaMode: 'premultiplied', // or 'opaque' if transparency is not needed from the canvas itself
                });

                // Create a sampler for texture sampling in shaders
                sampler = device.createSampler({
                    magFilter: 'linear', // How to sample when texture is magnified
                    minFilter: 'linear', // How to sample when texture is minified
                });

                // Create shader modules from WGSL code
                const vertexModule = device.createShaderModule({ code: vertexShaderWGSL });
                const fragmentModule = device.createShaderModule({ code: fragmentShaderWGSL_Grayscale });

                // Create the render pipeline
                pipeline = device.createRenderPipeline({
                    layout: 'auto', // Let WebGPU infer the bind group layout from shaders
                    vertex: {
                        module: vertexModule,
                        entryPoint: 'vs_main', // Entry function in the vertex shader
                    },
                    fragment: {
                        module: fragmentModule,
                        entryPoint: 'fs_main', // Entry function in the fragment shader
                        targets: [{ format: presentationFormat }], // Output format matches canvas
                    },
                    primitive: {
                        topology: 'triangle-list', // We are drawing triangles
                    },
                });
                
                updateStatus("WebGPU Initialized. Ready to load an image.", "success");
                canvasPlaceholder.textContent = "Output will appear here"; // Reset placeholder
                return true;

            } catch (error) {
                console.error("WebGPU Initialization Error:", error);
                updateStatus(`WebGPU Init Error: ${error.message}`, "error");
                processButton.disabled = true;
                return false;
            }
        }

        // --- Image Handling ---
        imageUpload.addEventListener('change', async (event) => {
            const file = event.target.files[0];
            if (!file) {
                originalImage.classList.add('hidden');
                originalPlaceholder.classList.remove('hidden');
                originalPlaceholder.textContent = "No image loaded";
                processButton.disabled = true;
                return;
            }

            try {
                // Create an ImageBitmap from the file for efficient texture uploading
                sourceImageBitmap = await createImageBitmap(file);

                // Display original image
                originalImage.src = URL.createObjectURL(file);
                originalImage.classList.remove('hidden');
                originalPlaceholder.classList.add('hidden');

                // Prepare canvas and WebGPU texture
                webgpuCanvas.width = sourceImageBitmap.width;
                webgpuCanvas.height = sourceImageBitmap.height;
                
                // Ensure canvas placeholder is managed correctly
                if (webgpuCanvas.width > 0 && webgpuCanvas.height > 0) {
                    canvasPlaceholder.classList.add('hidden');
                } else {
                    canvasPlaceholder.classList.remove('hidden');
                    canvasPlaceholder.textContent = "Output will appear here";
                }


                if (!device) {
                    updateStatus("WebGPU not initialized. Cannot prepare image texture.", "error");
                    processButton.disabled = true;
                    return;
                }

                // Destroy previous texture if it exists to free GPU memory
                if (sourceTexture) {
                    sourceTexture.destroy();
                }

                // Create a WebGPU texture to hold the image data
                sourceTexture = device.createTexture({
                    size: [sourceImageBitmap.width, sourceImageBitmap.height, 1], // Width, Height, Depth
                    format: 'rgba8unorm', // Standard format for images (8-bit R,G,B,A, unsigned normalized)
                    usage: GPUTextureUsage.TEXTURE_BINDING |  // Texture can be bound in a bind group
                           GPUTextureUsage.COPY_DST |         // Data can be copied into this texture
                           GPUTextureUsage.RENDER_ATTACHMENT, // Can be used as a render target (might not be strictly needed here but good practice)
                });

                // Copy the ImageBitmap data to the WebGPU texture
                device.queue.copyExternalImageToTexture(
                    { source: sourceImageBitmap }, // Source of the image data
                    { texture: sourceTexture },    // Destination texture
                    [sourceImageBitmap.width, sourceImageBitmap.height] // Size of the copy
                );
                
                // Create a Bind Group to link resources (texture, sampler) to the shader
                // This needs to be done *after* the sourceTexture is created and the pipeline is available.
                if (pipeline) { // Ensure pipeline is created before creating bind group
                    bindGroup = device.createBindGroup({
                        layout: pipeline.getBindGroupLayout(0), // Get layout from pipeline (group 0)
                        entries: [
                            { binding: 0, resource: sampler }, // Sampler at binding 0
                            { binding: 1, resource: sourceTexture.createView() }, // Texture view at binding 1
                        ],
                    });
                    processButton.disabled = false; // Enable processing button
                    updateStatus("Image loaded. Ready to process.", "info");
                } else {
                    updateStatus("Pipeline not ready. Cannot create bind group.", "error");
                    processButton.disabled = true;
                }

            } catch (err) {
                console.error("Error loading image:", err);
                updateStatus(`Error loading image: ${err.message}`, "error");
                originalImage.classList.add('hidden');
                originalPlaceholder.classList.remove('hidden');
                originalPlaceholder.textContent = "Error loading image";
                processButton.disabled = true;
            }
        });

        // --- Processing Logic ---
        processButton.addEventListener('click', () => {
            // Ensure all necessary WebGPU components are ready
            if (!device || !pipeline || !sourceTexture || !bindGroup || !context) {
                updateStatus("WebGPU components not ready. Cannot process.", "error");
                console.error("WebGPU components not ready:", {device, pipeline, sourceTexture, bindGroup, context});
                return;
            }
            if (!sourceImageBitmap) { // Also check if an image has been loaded
                updateStatus("No image loaded to process.", "warn");
                return;
            }

            try {
                updateStatus("Processing image...", "info");

                // Create a command encoder to record GPU commands
                const commandEncoder = device.createCommandEncoder();
                
                // Get the current texture view from the canvas context to render to
                const textureView = context.getCurrentTexture().createView();

                // Define the render pass
                const renderPassDescriptor = {
                    colorAttachments: [{
                        view: textureView, // The texture view to render to (our canvas)
                        clearValue: { r: 0.0, g: 0.0, b: 0.0, a: 0.0 }, // Clear color (transparent black)
                        loadOp: 'clear',   // Clear the attachment before rendering
                        storeOp: 'store',  // Store the results of the render pass
                    }],
                };

                // Begin the render pass
                const passEncoder = commandEncoder.beginRenderPass(renderPassDescriptor);
                passEncoder.setPipeline(pipeline); // Set the render pipeline to use
                passEncoder.setBindGroup(0, bindGroup); // Set the bind group for resources (texture, sampler)
                
                // Draw the full-screen quad (6 vertices, 1 instance, start at vertex 0, instance 0)
                passEncoder.draw(6, 1, 0, 0); 
                
                passEncoder.end(); // End the render pass

                // Submit the recorded commands to the GPU queue for execution
                device.queue.submit([commandEncoder.finish()]);

                updateStatus("Grayscale filter applied successfully!", "success");
                canvasPlaceholder.classList.add('hidden'); // Hide placeholder as canvas now has content
                webgpuCanvas.classList.remove('hidden'); // Ensure canvas is visible

            } catch (err) {
                console.error("Error processing image with WebGPU:", err);
                updateStatus(`Error during WebGPU processing: ${err.message}`, "error");
            }
        });

        // --- Utility Functions ---
        function updateStatus(message, type = "info") {
            webgpuStatus.textContent = message;
            // Clear previous styling
            webgpuStatus.classList.remove('bg-red-600', 'bg-green-600', 'bg-blue-600', 'bg-yellow-500', 'text-white', 'text-slate-800');
            
            switch (type) {
                case "error":
                    webgpuStatus.classList.add('bg-red-600', 'text-white');
                    break;
                case "success":
                    webgpuStatus.classList.add('bg-green-600', 'text-white');
                    break;
                case "warn":
                    webgpuStatus.classList.add('bg-yellow-500', 'text-slate-800');
                    break;
                case "info":
                default:
                    webgpuStatus.classList.add('bg-blue-600', 'text-white');
                    break;
            }
        }

        // --- Initialize WebGPU on page load ---
        document.addEventListener('DOMContentLoaded', async () => {
            // Initially hide canvas and show placeholder
            webgpuCanvas.width = 0; // Avoid tiny canvas render before image load
            webgpuCanvas.height = 0;
            canvasPlaceholder.classList.remove('hidden');
            originalPlaceholder.classList.remove('hidden');


            if (await initWebGPU()) {
                // WebGPU initialized successfully. UI will guide user.
            } else {
                // WebGPU initialization failed. Status message already updated.
                // Ensure process button remains disabled.
                processButton.disabled = true;
            }
        });

    </script>
</body>
</html>
